<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hive | 醉過方知酒濃]]></title>
  <link href="http://2naive.me/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://2naive.me/"/>
<<<<<<< HEAD
  <updated>2013-12-27T15:35:34+08:00</updated>
=======
  <updated>2013-12-28T03:01:53+08:00</updated>
>>>>>>> ui-dev
  <id>http://2naive.me/</id>
  <author>
    <name><![CDATA[Eli]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[在hive中使用python来做UDF有点糟]]></title>
    <link href="http://2naive.me/blog/2013/11/01/udf-in-python-sucks/"/>
    <updated>2013-11-01T23:57:00+08:00</updated>
    <id>http://2naive.me/blog/2013/11/01/udf-in-python-sucks</id>
    <content type="html"><![CDATA[<p>之前写了篇介绍如何在HIVE中用python来写UDF的文字，最近终于也开始需要用UDF来处理一些数据了。</p>

<p>可是现实和理想总是存在些差距，原本想着python简洁的代码和随改随用的特性，在UDF的灵活性上显得有点苍白无力。利用<code>TRANSFORM</code>, <code>USING</code>, <code>AS</code>的结构来把python的脚本当做UDF，虽然看起来非常方便，但实际用起来却有很大问题。</p>

<h2 id="section">无法单独处理某个字段</h2>
<p>没有仔细研究过这个具体的流程，不过猜想应该是从hive中select出来的数据以pipe的形式转到python的stdin中，再把python中的stdout的内容转到hive的输出中。这样如果仅需对选出的某个字段进行处理，其余字段不进行处理的话，必须在Python脚本中注意当前要处理的字段是第几个，一旦select中的顺序有所改变，那python脚本应该就无法处理了。目前仅选一个字段出来进行处理的情况比较少，大部分都是选出好几个字段来看。所以虽然python很好，不过我还是“十动然拒”了吧。</p>

<h2 id="section-1">无法外加聚集函数</h2>
<p>在使用中，下面这种格式是很死的。</p>

<p><code>sql
SELECT TRANSFORM(field_A, field_B) USING 'python foo.py' AS(field_AA, field_BB) FROM ...
</code></p>

<p>比如你想再对UDF外面加一层<code>Count</code>是没有办法实现的。某人会告诉你格式不对，无法运行。但是常规的用java写的UDF是没有问题的，全部都能hold住~</p>

<h2 id="section-2">外加的吐槽</h2>
<p>实际工作中，不仅仅需要<code>UDF</code>，<code>UDAF</code>和<code>UDTF</code>都是会经常遇到的。而Python拼死只能处理UDF的情况，对于后者只能呵呵了。于是反正UDAF和UDTF还是需要JAVA来写，那UDF也就顺便用JAVA写了吧。这样代码也统一一点，便于管理。</p>

<p>所以虽然python在UDF上的效果可以实现，不过功能太过简单，实际环境下还是很无力的。接下来还是用JAVA写<code>UDF</code>,<code>UDAF</code>和<code>UDTF</code>吧。回头吧，少年~</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HIVE中使用python实现UDF]]></title>
    <link href="http://2naive.me/blog/2013/09/25/hive-python-udf/"/>
    <updated>2013-09-25T00:00:00+08:00</updated>
    <id>http://2naive.me/blog/2013/09/25/hive-python-udf</id>
    <content type="html"><![CDATA[<p>HIVE，FACEBOOK的一个开源项目，利用类SQL的语句（HiveQL）来加快一般的MapReduce的开发过程。</p>

<p>UDF，user defined function, 因为HIVE毕竟不是一般的关系型数据库，支持的HQL有限，如果要实现复杂的功能，就要通过自己定义函数来帮助实现。</p>

<p>HIVE应该利用PIPE的原理，将自己查询的结果放到python脚本的stdin中。所以他的查询结果不会显示在terminal中，terminal中显示的结果是python的执行结果。</p>

<p>使用HIVE的命令进入数据仓库(search)</p>

<p><code>
use search;
</code></p>

<p>使用HIVE的命令查看已经建立的表</p>

<p><code>
show tables;
</code></p>

<p>使用HIVE的命令查看xxx表中的字段</p>

<p><code>
describe xxx;
</code></p>

<p>使用HIVE命令用PYTHON实现UDF</p>

<p><code>
add file udf.py;
SELECT 
TRANSFORM(keyword)
USING 'python udf.py'
AS(keyword)
FROM xxx
WHERE dt='2013-09-25'
;
</code></p>

<p>要注意的是，这里的TRANSFORM的内容可以写*，但是AS()里就不能写*，会报错。</p>

<p>输入到python中的内容，是按照AS里的数量来决定的。</p>

<p>下面是python的脚本，内容很简单，就是把输入的东西原封不动输出来。</p>

<p><code>python udf.py
import sys
for line in sys.stdin:
    line = line.strip()
    print line
</code></p>
]]></content>
  </entry>
  
</feed>
