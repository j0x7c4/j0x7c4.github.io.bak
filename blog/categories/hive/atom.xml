<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hive | 醉過方知酒濃]]></title>
  <link href="http://2naive.me/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://2naive.me/"/>
  <updated>2014-04-25T18:37:01+08:00</updated>
  <id>http://2naive.me/</id>
  <author>
    <name><![CDATA[Eli]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[用python写hive的UDAF]]></title>
    <link href="http://2naive.me/blog/2014/01/05/hive-stream-with-python/"/>
    <updated>2014-01-05T15:33:00+08:00</updated>
    <id>http://2naive.me/blog/2014/01/05/hive-stream-with-python</id>
    <content type="html"><![CDATA[<p>之前写过一篇关于说Python无法实现hive UDAF的文章，后来经过尝试，python完全可以代替JAVA实现hive中常用的UDF,UDAF,及UDTF.</p>

<p>通过<code>transform</code>语句，可以将hive语句中<code>select</code>得到的数据通过类似hadoop中<a href="http://hadoop.apache.org/docs/stable1/streaming.html">streaming</a>的方式，传给<code>using</code>语句中的脚本作为输入。用<code>DISTRIBUTE BY</code>将mapper的输出分类给reducer. </p>

<h2 id="udaf">UDAF</h2>

<p>由于<code>SELECT</code>和<code>DISTRIBUTE BY</code>是有先后顺序，所以要先得到经过<code>DISTRIBUTE BY</code>的数据，再用<code>TRANSFORM</code>进行转化，保证相同的key落入到同一个reducer中去。reducer把处理后的结果以<code>AS</code>中的字段格式进行输出。</p>

<p><code>
FROM (
SELECT a,b,c,d
FROM db
DISTRIBUTE BY a
SORT BY a,b ) T
SELECT TRANSFORM(T.a, T.b, T.c, T.d)
USING 'python udaf.py'
AS (a,b,e)
</code> </p>

<p>``` python
#! /usr/bin/env python
#encoding=utf-8
import sys
import logging</p>

<p>current={“a”:None,”b”:None,”e”:None}
a=None</p>

<p>def foo (c,d):
    e = ‘’
    #To-do, write something
    return e</p>

<p>def output ():
    #To-do, write something
    print …</p>

<p>for l in sys.stdin:
    try:
        a,b,c,d = l.strip().split(‘\t’)
        e=foo(c,d)
    except:
        logging.error(l)
        continue</p>

<pre><code>if current["a"] != a:
    if current["a"]: 
        output()
    current["a"]=a
    current["b"]=b
    current["e"]=e
else:
    current["e"]+=e
</code></pre>

<p>if current[“a”]:
    output()
```</p>

<h2 id="section">代码的重用</h2>

<p>如果按照以上的方式，是可以实现UDAF的效果，但是代码的重用性不高。如果在java中，可以写多个不同功能的类，然后打在一个jar包中。不过在python中实现起来有点困难，比如我在另一个py脚本中定义两个函数foo,bar,在一个udaf脚本中需要先import那个py 模块，然后再用这两个函数。这样就需要在<code>add file</code>中同时添加两个py脚本，在实际使用过程中，发现添加多个py脚本后，在py代码中使用import语句，会找不到那个文件的路径。原因是在hdfs中，这些py脚本不一定会在同一个目录下。所以只能把所有需要用到的函数写在同一个py脚本中。这样一来，以后需要同种功能的函数时，需要重写这些代码。之后再看看有没有其他可行的方案。</p>

<p><a href="{{root_url}}/blog/2013/09/25/hive-python-udf/">HIVE中使用python实现UDF</a></p>

<p><a href="{{root_url}}/blog/2013/11/01/udf-in-python-sucks/">在hive中使用python来做UDF有点糟</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在hive中使用python来做UDF有点糟]]></title>
    <link href="http://2naive.me/blog/2013/11/01/udf-in-python-sucks/"/>
    <updated>2013-11-01T23:57:00+08:00</updated>
    <id>http://2naive.me/blog/2013/11/01/udf-in-python-sucks</id>
    <content type="html"><![CDATA[<p>之前写了篇介绍如何在HIVE中用python来写UDF的文字，最近终于也开始需要用UDF来处理一些数据了。</p>

<p>可是现实和理想总是存在些差距，原本想着python简洁的代码和随改随用的特性，在UDF的灵活性上显得有点苍白无力。利用<code>TRANSFORM</code>, <code>USING</code>, <code>AS</code>的结构来把python的脚本当做UDF，虽然看起来非常方便，但实际用起来却有很大问题。</p>

<h2 id="section">无法单独处理某个字段</h2>
<p>没有仔细研究过这个具体的流程，不过猜想应该是从hive中select出来的数据以pipe的形式转到python的stdin中，再把python中的stdout的内容转到hive的输出中。这样如果仅需对选出的某个字段进行处理，其余字段不进行处理的话，必须在Python脚本中注意当前要处理的字段是第几个，一旦select中的顺序有所改变，那python脚本应该就无法处理了。目前仅选一个字段出来进行处理的情况比较少，大部分都是选出好几个字段来看。所以虽然python很好，不过我还是“十动然拒”了吧。</p>

<h2 id="section-1">无法外加聚集函数</h2>
<p>在使用中，下面这种格式是很死的。</p>

<p><code>sql
SELECT TRANSFORM(field_A, field_B) USING 'python foo.py' AS(field_AA, field_BB) FROM ...
</code></p>

<p>比如你想再对UDF外面加一层<code>Count</code>是没有办法实现的。某人会告诉你格式不对，无法运行。但是常规的用java写的UDF是没有问题的，全部都能hold住~</p>

<h2 id="section-2">外加的吐槽</h2>
<p>实际工作中，不仅仅需要<code>UDF</code>，<code>UDAF</code>和<code>UDTF</code>都是会经常遇到的。而Python拼死只能处理UDF的情况，对于后者只能呵呵了。于是反正UDAF和UDTF还是需要JAVA来写，那UDF也就顺便用JAVA写了吧。这样代码也统一一点，便于管理。</p>

<p>所以虽然python在UDF上的效果可以实现，不过功能太过简单，实际环境下还是很无力的。接下来还是用JAVA写<code>UDF</code>,<code>UDAF</code>和<code>UDTF</code>吧。回头吧，少年~</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HIVE中使用python实现UDF]]></title>
    <link href="http://2naive.me/blog/2013/09/25/hive-python-udf/"/>
    <updated>2013-09-25T00:00:00+08:00</updated>
    <id>http://2naive.me/blog/2013/09/25/hive-python-udf</id>
    <content type="html"><![CDATA[<p>HIVE，FACEBOOK的一个开源项目，利用类SQL的语句（HiveQL）来加快一般的MapReduce的开发过程。</p>

<p>UDF，user defined function, 因为HIVE毕竟不是一般的关系型数据库，支持的HQL有限，如果要实现复杂的功能，就要通过自己定义函数来帮助实现。</p>

<p>HIVE应该利用PIPE的原理，将自己查询的结果放到python脚本的stdin中。所以他的查询结果不会显示在terminal中，terminal中显示的结果是python的执行结果。</p>

<p>使用HIVE的命令进入数据仓库(search)</p>

<p><code>
use search;
</code></p>

<p>使用HIVE的命令查看已经建立的表</p>

<p><code>
show tables;
</code></p>

<p>使用HIVE的命令查看xxx表中的字段</p>

<p><code>
describe xxx;
</code></p>

<p>使用HIVE命令用PYTHON实现UDF</p>

<p><code>
add file udf.py;
SELECT 
TRANSFORM(keyword)
USING 'python udf.py'
AS(keyword)
FROM xxx
WHERE dt='2013-09-25'
;
</code></p>

<p>要注意的是，这里的TRANSFORM的内容可以写*，但是AS()里就不能写*，会报错。</p>

<p>输入到python中的内容，是按照AS里的数量来决定的。</p>

<p>下面是python的脚本，内容很简单，就是把输入的东西原封不动输出来。</p>

<p><code>python udf.py
import sys
for line in sys.stdin:
    line = line.strip()
    print line
</code></p>
]]></content>
  </entry>
  
</feed>
